{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\tdef __init__(self, parameters: np.ndarray) -> None:\n",
    "\t\tif parameters.shape != (13002, 1):\n",
    "\t\t\traise ValueError('parameters shape should be (13002, 1)!')\n",
    "\t\t\n",
    "\t\tself.parameters = parameters\n",
    "\n",
    "\t\tcount = 0\n",
    "\t\t\n",
    "\t\t# weights1 is of size (16)x(28*28)\n",
    "\t\ttotal_size = 16*28*28\n",
    "\t\tself.weights1 = parameters[count:count+total_size].reshape(16, 28*28)\n",
    "\n",
    "\t\tcount += total_size \n",
    "\n",
    "\t\t# biases1 is of size 16x1\n",
    "\t\ttotal_size = 16*1\n",
    "\t\tself.biases1 = parameters[count:count+total_size].reshape(16, 1)\n",
    "\n",
    "\t\tcount += total_size \n",
    "\n",
    "\t\t# weights2 is of size 16x16\n",
    "\t\ttotal_size = 16*16\n",
    "\t\tself.weights2 = parameters[count:count+total_size].reshape(16, 16)\n",
    "\n",
    "\t\tcount += total_size \n",
    "\n",
    "\t\t# biases2 is of size 16x1\n",
    "\t\ttotal_size = 16*1\n",
    "\t\tself.biases2 = parameters[count:count+total_size].reshape(16, 1)\n",
    "\t\t\n",
    "\t\tcount += total_size \n",
    "\n",
    "\t\t# weights3 is of size 10*16\n",
    "\t\ttotal_size = 10*16\n",
    "\t\tself.weights3 = parameters[count:count+total_size].reshape(10, 16)\n",
    "\n",
    "\t\tcount += total_size \n",
    "\n",
    "\t\t# biases3 is of size 10*1\n",
    "\t\ttotal_size = 10*1\n",
    "\t\tself.biases3 = parameters[count:count+total_size].reshape(10, 1)\n",
    "\n",
    "\t\tself.input_layer = np.zeros((784, 1))\n",
    "\t\t\n",
    "\t\t# inner hidden layers of the network\n",
    "\t\tself.z_hlayer1 = np.zeros((16, 1))\n",
    "\t\tself.hlayer1 = NeuralNetwork.sigmoid(self.z_hlayer1)\n",
    "\n",
    "\t\tself.z_hlayer2 = np.zeros((16, 1))\n",
    "\t\tself.hlayer2 = NeuralNetwork.sigmoid(self.z_hlayer2)\n",
    "\n",
    "\t\tself.z_output_layer = np.zeros((10, 1))\n",
    "\t\tself.output_layer = NeuralNetwork.sigmoid(self.z_output_layer)\n",
    "\n",
    "\t\tdel total_size\n",
    "\t\tdel count\n",
    "\n",
    "\n",
    "\tdef load_input_layer(self, input_vector: np.ndarray) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\t\tLaad the input handwritten digit\n",
    "\t\t\tinput_vector: np.ndarray of shape (784, 1)\n",
    "\t\t\tthese numbers are between 0-255\n",
    "\t\t\tthe function will squish them between 0-1\n",
    "\t\t\"\"\"\n",
    "\t\tself.input_layer = (input_vector / 255)\n",
    "\t\t\n",
    "\n",
    "\tdef cost_of_single_sample(self, sample: np.ndarray, true_label: int) -> float:\n",
    "\t\tself.load_input_layer(input_vector=sample)\n",
    "\t\tself.feed_forward()\n",
    "\t\t\n",
    "\t\t# construct the output vector based on the label\n",
    "\t\tdesired_output = np.zeros((10, 1))\n",
    "\t\tdesired_output[true_label] = 1.0\n",
    "\t\t\n",
    "\t\t# compare the self.output_layer and the desired_output\n",
    "\t\t# using mean squared error\n",
    "\t\tcost = np.sum((self.output_layer - desired_output)**2)\n",
    "\t\treturn cost\n",
    "\t\n",
    "\n",
    "\tdef cost_of_test_data(self, test_samples: np.ndarray, true_labels: np.ndarray) -> float:\n",
    "\t\t\"\"\"\n",
    "\t\t\tsamples.shape = (784, m)\n",
    "\t\t\ttrue_labels.shape = (1, m)\n",
    "\t\t\tsamples: is a np array which each col represents one sample, and \n",
    "\t\t\teach col has 784 numbers in them, the pixel values\n",
    "\t\t\t\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tMSE: float = 0\n",
    "\t\tfor sample, label in zip(test_samples.T, true_labels[0]):\n",
    "\t\t\tMSE += self.cost_of_single_sample(sample, label)\n",
    "\t\t\n",
    "\t\tM = len(test_samples.T)\n",
    "\t\tMSE = (1 / M) * MSE\n",
    "\t\treturn MSE\n",
    "\t\n",
    "\n",
    "\tdef accuracy_score(self, test_samples: np.ndarray, true_labels: np.ndarray) -> float:\n",
    "\t\t#* test_samples.shape = (784, m)\n",
    "\t\t#* true_labels.shape = (1, m)\n",
    "\n",
    "\t\ttotal: int = len(test_samples.T)\n",
    "\t\ttrues: int = 0\n",
    "\n",
    "\t\tfor sample, label in zip(test_samples.T, true_labels[0]):\n",
    "\t\t\tresult = self.predict(sample)\n",
    "\t\t\tif result == label:\n",
    "\t\t\t\ttrues += 1\n",
    "\t\t\n",
    "\t\treturn (trues / total)\n",
    "\n",
    "\n",
    "\tdef predict_v(self, sample: np.ndarray) -> np.ndarray:\n",
    "\t\t#* sample.shape = (784, 1)\n",
    "\t\tself.load_input_layer(input_vector=sample)\n",
    "\t\tself.feed_forward()\n",
    "\t\t\n",
    "\t\treturn self.output_layer\n",
    "\n",
    "\t\n",
    "\tdef predict(self, sample) -> int:\n",
    "\t\toutput_vector = self.predict_v(sample)\n",
    "\t\treturn np.argmax(output_vector)\n",
    "\n",
    "\n",
    "\tdef backprop_one_sample(self, sample: np.ndarray, label: int) -> tuple:\n",
    "\t\t\"\"\"\n",
    "\t\t\tThis method holds all the math and calculus behind backpropagation\n",
    "\t\t\tit calculates the derivitive of the cost w.r.t all the weights and\n",
    "\t\t\tbiases of the network, for only one training data\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.load_input_layer(input_vector=sample)\n",
    "\t\tself.feed_forward()\n",
    "\n",
    "\t\t#* convert the label in int format into a one-hot vector\n",
    "\t\tdesired_output = np.zeros(self.output_layer.shape)\n",
    "\t\tdesired_output[label] = 1.0\n",
    "\t\t\n",
    "\t\t#* d_cost_p_ol.shape = (10, 1)\n",
    "\t\td_cost_p_ol = 2 * (self.output_layer - desired_output)\n",
    "\n",
    "\t\t#* d_sigmoid(z_ol)\n",
    "\t\t#* times the gradient of the cost w.r.t activations of the output layer\n",
    "\t\t#* error_ol.shape = (10, 1)\n",
    "\t\terror_ol = NeuralNetwork.d_sigmoid(self.z_output_layer) *  d_cost_p_ol\n",
    "\n",
    "\t\t#* d_sigmoid(self.z_hlayer2).shape = (16, 1)\n",
    "\t\t#* self.weights3.T.shape = (10, 16).T -> (16, 10)\n",
    "\t\t#* error_ol.shape = (10, 1)\n",
    "\t\t#* error_hl2.shape = (16, 1)\n",
    "\t\terror_hl2 = NeuralNetwork.d_sigmoid(self.z_hlayer2) * (self.weights3.T @ error_ol)\n",
    "\n",
    "\n",
    "\t\t#* d_sigmoid(self.z_hlayer1).shape = (16, 1)\n",
    "\t\t#* self.weights2.T.shape = (16, 16).T -> (16, 16)\n",
    "\t\t#* error_hl2.shape = (16, 1)\n",
    "\t\t#* error_hl1.shape = (16, 1)\n",
    "\t\terror_hl1 = NeuralNetwork.d_sigmoid(self.z_hlayer1) * (self.weights2.T @ error_hl2)\n",
    "\t\t\n",
    "\t\t#* d_cost_p_b3.shape = output_layer.shape = (10, 1)\n",
    "\t\td_cost_p_b3 = error_ol\n",
    "\t\n",
    "\n",
    "\t\t#* d_cost_p_b2.shape = hlayer2.shape = (16, 1)\n",
    "\t\td_cost_p_b2 = error_hl2\n",
    "\n",
    "\n",
    "\t\t#* d_cost_p_b1.shape = hlayer1.shape = (16, 1)\n",
    "\t\td_cost_p_b1 = error_hl1\n",
    "\n",
    "\t\t#* the derivative of the cost wr to the weights of the layer l will be\n",
    "\t\t#* the matrix mult of error of layer l and activation of layer l-1 transposed\n",
    "\t\t\n",
    "\t\t#* error_ol.shape = (10, 1)\n",
    "\t\t#* self.hlayer2.T.shape = (16, 1).T = (1, 16)\n",
    "\t\t#* d_cost_p_w3.shape = (10, 16)\n",
    "\t\td_cost_p_w3 = (error_ol @ self.hlayer2.T)\n",
    "\t\t\n",
    "\n",
    "\t\t#* error_hl2.shape = (16, 1)\n",
    "\t\t#* self.hlayer1.T.shape = (16, 1).T = (1, 16)\n",
    "\t\t#* d_cost_p_w2.shape = (16, 16)\n",
    "\t\td_cost_p_w2 = (error_hl2 @ self.hlayer1.T)\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t#* error_hl1.shape = (16, 1)\n",
    "\t\t#* self.input_layer.T.shape = (784, 1).T = (1, 784)\n",
    "\t\t#* d_cost_p_w1.shape = (16, 784)\n",
    "\t\td_cost_p_w1 = (error_hl1 @ self.input_layer.T)\n",
    "\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\td_cost_p_w1,\n",
    "\t\t\td_cost_p_b1,\n",
    "\t\t\td_cost_p_w2,\n",
    "\t\t\td_cost_p_b2,\n",
    "\t\t\td_cost_p_w3,\n",
    "\t\t\td_cost_p_b3\n",
    "\t\t)\n",
    "\n",
    "\t\n",
    "\tdef backpropagation(self, x_train: np.ndarray, y_train: np.ndarray) -> tuple:\n",
    "\t\t\"\"\"\n",
    "\t\t\tThis method will run the backprop_one_sample method for a dataset and \n",
    "\t\t\ttake the average of all the gradients of the weights and biases\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t#* m training samples\n",
    "\t\t#* x_train.shape = (784, m)\n",
    "\t\t#* y_train.shape = (1, m)\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tdw1 = np.zeros(self.weights1.shape)\n",
    "\t\tdb1 = np.zeros(self.biases1.shape)\n",
    "\t\t\n",
    "\t\tdw2 = np.zeros(self.weights2.shape)\n",
    "\t\tdb2 = np.zeros(self.biases2.shape)\n",
    "\t\t\n",
    "\t\tdw3 = np.zeros(self.weights3.shape)\n",
    "\t\tdb3 = np.zeros(self.biases3.shape)\n",
    "\n",
    "\t\t\n",
    "\t\tfor features, label in zip(x_train.T, y_train[0]):\n",
    "\t\t\t#* label: int\n",
    "\t\t\t#* features.shape = (784, 1)\n",
    "\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t#* label in this method should be an int\n",
    "\t\t\t(\n",
    "\t\t\t\ttdw1, \n",
    "\t\t\t\ttdb1, \n",
    "\t\t\t\ttdw2, \n",
    "\t\t\t\ttdb2, \n",
    "\t\t\t\ttdw3, \n",
    "\t\t\t\ttdb3\n",
    "\t\t\t) = self.backprop_one_sample(sample=features, label=label)\n",
    "\t\t\t\n",
    "\t\t\tdw1 += tdw1\n",
    "\t\t\tdb1 += tdb1\n",
    "\t\t\tdw2 += tdw2\n",
    "\t\t\tdb2 += tdb2\n",
    "\t\t\tdw3 += tdw3\n",
    "\t\t\tdb3 += tdb3\n",
    "\n",
    "\t\t#* now the dw1, db1, dw2, db2, dw3, db3 contain the sum of the derivatives of \n",
    "\t\t#* the samples inside the training data\n",
    "\t\t#* now they should be divided by the number of the train sample size, so dw and db, be an average\n",
    "\t\ttrain_data_size = x_train.shape[1]\n",
    "\t\t\n",
    "\t\tdw1 /= train_data_size\n",
    "\t\tdb1 /= train_data_size\n",
    "\t\tdw2 /= train_data_size\n",
    "\t\tdb2 /= train_data_size\n",
    "\t\tdw3 /= train_data_size\n",
    "\t\tdb3 /= train_data_size\n",
    "\t\t\n",
    "\t\t#* now they contain the gradient of the provided dataset\n",
    "\t\treturn (\n",
    "\t\t\tdw1,\n",
    "\t\t\tdb1,\n",
    "\t\t\tdw2,\n",
    "\t\t\tdb2,\n",
    "\t\t\tdw3,\n",
    "\t\t\tdb3\n",
    "\t\t)\n",
    "\t\t\t\n",
    "\n",
    "\tdef update_parameters(self, dw1, db1, dw2, db2, dw3, db3, learning_rate) -> None:\n",
    "\t\tself.weights1 -= learning_rate * dw1\n",
    "\t\tself.biases1 -= learning_rate * db1\n",
    "\t\t\n",
    "\t\tself.weights2 -= learning_rate * dw2\n",
    "\t\tself.biases2 -= learning_rate * db2\n",
    "\n",
    "\t\tself.weights3 -= learning_rate * dw3\n",
    "\t\tself.biases3 -= learning_rate * db3\n",
    "\n",
    "\n",
    "\tdef train(\n",
    "\t\t\tself, \n",
    "\t\t\tx_train: np.ndarray, \n",
    "\t\t\ty_train: np.ndarray,\n",
    "\t\t\t*,\n",
    "\t\t\tlearning_rate: float = 0.1,\n",
    "\t\t\tconstant_lr: bool = False,\n",
    "\t\t\tnumber_of_epochs: int = 80,\n",
    "\t\t\tmini_batches_size: int = 100\n",
    "\t) -> None:\n",
    "\t\t\"\"\"Trains the model with the labeled training data\"\"\"\n",
    "\n",
    "\t\t#* initialize the parameters randomly\n",
    "\t\t#self.parameters = np.random.normal(size=(13002, 1))\n",
    "\t\t\n",
    "\t\t#* m training samples\n",
    "\t\t#* x_train.shape = (784, m)\n",
    "\t\t#* y_train.shape = (1, m)\n",
    "\t\t\n",
    "\t\t#* first we'd better attach the x_train and y_train together\n",
    "\t\t#* then we can shuffle the training data\n",
    "\t\t#* and then seperate the x and y again\n",
    "\t\t#* add y row to the 0-th row of train_data\n",
    "\t\ttrain_data = np.vstack((y_train, x_train))\n",
    "\n",
    "\t\t#! now because shuffle, shuffles the array based on the rows\n",
    "\t\t#! but we need to shuffle the data based on the coloumns\n",
    "\t\t#! we have to transpose it twice to get around this\n",
    "\t\ttrain_data = train_data.T\n",
    "\t\tnp.random.shuffle(train_data)\n",
    "\t\ttrain_data = train_data.T\n",
    "\n",
    "\t\t#* now that the data is shuffled properly\n",
    "\t\t#* we should divide the data into mini-batches\n",
    "\t\tmini_batches: list[np.ndarray] = []\n",
    "\t\tcount: int = 0\n",
    "\t\t\n",
    "\t\twhile True:\n",
    "\t\t\tif count >= len(train_data.T):\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\t#mini_batches.append(train_data.T[count:count+mini_batches_size].T)\n",
    "\t\t\tmini_batches.append(train_data[:, count:count+mini_batches_size])\n",
    "\t\t\tcount += mini_batches_size\n",
    "\t\t\n",
    "\t\tinitial_lr: float = learning_rate\n",
    "\t\tdecay_rate: float = 0.1\n",
    "\t\t#for epoch in range(number_of_epochs):\n",
    "\t\t#* now each mini-batch corresponds to one step at gradient descent\n",
    "\t\t#* batch.shape = (784+1, mini_batches_size), the label and the features\n",
    "\t\tfor batch in mini_batches:\n",
    "\t\t\t#* batch.shape = (785, mini_batches_size)\n",
    "\t\t\tx_train_batch = batch[1:] # (784, mini_batches_size)\n",
    "\t\t\ty_train_batch = batch[0:1] # (1, mini_batches_size)\n",
    "\t\t\t\n",
    "\t\t\t#* the backprop algorithm will run for each batch, one step downhill towards a local minima\n",
    "\t\t\tdw1, db1, dw2, db2, dw3, db3 = self.backpropagation(x_train_batch, y_train_batch)\n",
    "\n",
    "\t\t\tif constant_lr:\n",
    "\t\t\t\tlr = learning_rate\n",
    "\t\t\telse:\n",
    "\t\t\t\tlr = np.exp(-epoch * decay_rate) * initial_lr\n",
    "\n",
    "\n",
    "\t\t\t#* change each of the weights and biases accordingly\n",
    "\t\t\tself.update_parameters(dw1, db1, dw2, db2, dw3, db3, learning_rate=lr)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t#* update the self.output_layer and consequently the cost\n",
    "\t\t\t#self.feed_forward()\n",
    "\n",
    "\t\t\t#print(f'{self.accuracy_score(x_train, y_train)*100:.2f}%')\n",
    "\t\t\t\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef sigmoid(x: float | np.ndarray) -> float:\n",
    "\t\treturn 1 / (1 + np.exp(-x))\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef d_sigmoid(x: float | np.ndarray) -> float:\n",
    "\t\treturn np.exp(-x) / (np.pow((1 + np.exp(-x)), 2))\n",
    "\n",
    "\n",
    "\tdef feed_forward(self) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\t\tWill calculate all the values in all the layers \n",
    "\t\t\tbased on the weights and biases \n",
    "\t\t\"\"\"\n",
    "\t\t# hidden layer 1 has 16 neurons -> shape: (16, 1)\n",
    "\t\t# weights1.shape = (16, 784)\n",
    "\t\t# 16*784\t784*1 + 16,1\n",
    "\t\tself.input_layer = self.input_layer.reshape((784, 1))\n",
    "\n",
    "\t\tself.z_hlayer1 = self.weights1 @ self.input_layer + self.biases1\n",
    "\t\tself.hlayer1 = NeuralNetwork.sigmoid(self.z_hlayer1)\n",
    "\n",
    "\t\t# hidden layer 2 has 16 neurons -> shape: (16, 1)\n",
    "\t\tself.z_hlayer2 = self.weights2 @ self.hlayer1 + self.biases2\n",
    "\t\tself.hlayer2 = NeuralNetwork.sigmoid(self.z_hlayer2)\n",
    "\t\t\n",
    "\t\t# output layer has 10 neurons -> shape(10, 1)\n",
    "\t\t# one neuron for each digit\n",
    "\t\tself.z_output_layer = self.weights3 @ self.hlayer2 + self.biases3\n",
    "\t\tself.output_layer = NeuralNetwork.sigmoid(self.z_output_layer)\n",
    "\n",
    "\n",
    "\t\tif self.output_layer.shape != (10, 1):\n",
    "\t\t\traise ValueError(f'{self.output_layer.shape} not a correct shape!')\n",
    "\n",
    "\n",
    "\tdef print_network(self, hidden_layers = False) -> None:\n",
    "\t\tif hidden_layers:\n",
    "\t\t\tprint('hLayer 1:')\n",
    "\t\t\tprint(self.hlayer1)\n",
    "\t\t\t\n",
    "\t\t\tprint('hLayer 2:')\n",
    "\t\t\tprint(self.hlayer2)\n",
    "\n",
    "\t\tprint('Output Layer:')\n",
    "\t\tprint(self.output_layer)\n",
    "\n",
    "\n",
    "\tdef plot_network(self):\n",
    "\t\tlayers = [\n",
    "\t\t\tself.hlayer1.reshape((16,)),\n",
    "\t\t\tself.hlayer2.reshape((16,)),\n",
    "\t\t\tself.output_layer.reshape((10,))\t\t\n",
    "\t\t]\n",
    "\n",
    "\t\tfig, ax = plt.subplots()\n",
    "\t\t\n",
    "\t\t# Determine the positions for the layers\n",
    "\t\tlayer_sizes = [len(layer) for layer in layers]\n",
    "\t\tv_spacing = max(layer_sizes) + 1 + 3  # Vertical spacing between neurons\n",
    "\t\th_spacing = 15  # Horizontal spacing between layers\n",
    "\t\t\n",
    "\t\t# Iterate over layers\n",
    "\t\tfor i, layer in enumerate(layers):\n",
    "\t\t\ty_pos = np.linspace(v_spacing / 2, -v_spacing / 2, len(layer))\n",
    "\t\t\tx_pos = np.full_like(y_pos, i * h_spacing)\n",
    "\t\t\t\n",
    "\t\t\t# Plot neurons\n",
    "\t\t\tfor j, neuron_value in enumerate(layer):\n",
    "\t\t\t\tcircle = plt.Circle((x_pos[j], y_pos[j]), radius=0.4, facecolor=str(neuron_value), edgecolor='k')\n",
    "\t\t\t\tax.add_artist(circle)\n",
    "\t\t\t\t# Optional: Add text to show the neuron value\n",
    "\t\t\t\tax.text(x_pos[j]+1.5, y_pos[j], f'{neuron_value:.2f}', ha='center', va='center', color='black')\n",
    "\n",
    "\t\t\t# Draw connections (optional)\n",
    "\t\t\tif i > 0:\n",
    "\t\t\t\tprev_layer = layers[i - 1]\n",
    "\t\t\t\tprev_y_pos = np.linspace(v_spacing / 2, -v_spacing / 2, len(prev_layer))\n",
    "\t\t\t\tprev_x_pos = np.full_like(prev_y_pos, (i - 1) * h_spacing)\n",
    "\t\t\t\tfor k in range(len(prev_layer)):\n",
    "\t\t\t\t\tfor l in range(len(layer)):\n",
    "\t\t\t\t\t\tax.plot([prev_x_pos[k], x_pos[l]], [prev_y_pos[k], y_pos[l]], 'k-', lw=0.04)\n",
    "\n",
    "\t\tax.set_aspect('equal')\n",
    "\t\tax.axis('off')\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.show()\n",
    "\n",
    "\n",
    "\tdef show_sample(self, sample: np.ndarray):\n",
    "\t\t#* sample.shape = (784, 1)\n",
    "\t\tmatrix = np.array(sample).reshape(28, 28)\n",
    "\n",
    "\t\tplt.imshow(matrix, cmap='gray', vmin=0, vmax=255)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\n",
    "\tdef print_stat(self, x_test: np.ndarray, y_test: np.ndarray) -> None:\n",
    "\t\tscore = self.accuracy_score(x_test, y_test)\n",
    "\t\tcost = self.cost_of_test_data(x_test, y_test)\n",
    "\n",
    "\t\tprint(f'Accuracy = {score * 100:.2f}%')\n",
    "\t\tprint(f'Cost = {cost:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/mnist_train.csv')\n",
    "test_df = pd.read_csv('./data/mnist_test.csv')\n",
    "\n",
    "x_train = np.array(train_df.iloc[:, 1:]).transpose() # shape = 784 * m, so each col is a sample\n",
    "y_train = np.array(train_df.iloc[:, 0:1]).reshape((1, -1)) # shape = 1 * m\n",
    "\n",
    "x_test = np.array(test_df.iloc[:, 1:]).transpose()\n",
    "y_test = np.array(test_df.iloc[:, 0:1]).reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNetwork(parameters=np.random.normal(size=(13002, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 10.55%\n",
      "Cost = 2.581\n"
     ]
    }
   ],
   "source": [
    "NN.print_stat(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.train(x_train, y_train, number_of_epochs=5, mini_batches_size=100, learning_rate=0.05, constant_lr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 23.27%\n",
      "Cost = 0.890\n",
      "Accuracy = 22.99%\n",
      "Cost = 0.886\n"
     ]
    }
   ],
   "source": [
    "NN.print_stat(x_train, y_train)\n",
    "NN.print_stat(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "previous_score = NN.accuracy_score(x_test, y_test) * 100\n",
    "count = 0\n",
    "while True:\n",
    "\tNN.train(x_train, y_train, learning_rate=lr)\n",
    "\toos_accracy = NN.accuracy_score(x_test, y_test) * 100\n",
    "\tprint(f'{oos_accracy:.2f}%')\n",
    "\tif oos_accracy > previous_score:\n",
    "\t\tlr *= 1.01\n",
    "\t\tprevious_score = oos_accracy\n",
    "\telse:\n",
    "\t\tlr *= 0.97\n",
    "\t\n",
    "\tcount += 1\n",
    "\tif count == 100: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* examples where the prediction was not correct\n",
    "for i, (sample, label) in enumerate(zip(x_test.T, y_test[0])):\n",
    "\tp = NN.predict(sample)\n",
    "\tif p != label:\n",
    "\t\tNN.show_sample(sample)\n",
    "\t\tprint(f'true label: {label}')\n",
    "\t\tprint(f'prediction: {p}')\n",
    "\tif i >= 200: break\t\n",
    "\n",
    "layers = [NN.hlayer1.reshape((16,)), NN.hlayer2.reshape((16,)), NN.output_layer.reshape((10,))]\n",
    "#plot_neural_network(layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
