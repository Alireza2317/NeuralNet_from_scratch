{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Implementation From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections.abc import Callable\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\tdef __init__(\n",
    "\t\tself, \n",
    "\t\tlayers_structure: list[int], \n",
    "\t\tparameters: np.ndarray | None = None, \n",
    "\t\tactivation: list[str] | str = 'sigmoid'\n",
    "\t) -> None:\n",
    "\t\tself.layers_structure: list[int] = layers_structure\n",
    "\t\t\n",
    "\t\t# number of layers, except the input layer\n",
    "\t\tself._L: int = len(layers_structure) - 1\n",
    "\n",
    "\t\t# calculation of the correct number of parameters \n",
    "\t\tself._NUMBER_OF_PARAMS: int = 0\n",
    "\t\tfor i in range(self._L):\n",
    "\t\t\t# number of weights\n",
    "\t\t\tself._NUMBER_OF_PARAMS += self.layers_structure[i] * self.layers_structure[i+1]\n",
    "\n",
    "\t\t\t# number of biases\n",
    "\t\t\tself._NUMBER_OF_PARAMS += self.layers_structure[i+1]\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tself._weights_shapes: list[tuple[int]] = []\n",
    "\t\tself._biases_shapes: list[tuple[int]] = []\n",
    "\t\t# computing the appropriate shape of each weights matrix between layers\n",
    "\t\tfor i in range(self._L):\n",
    "\t\t\tself._weights_shapes.append((self.layers_structure[i+1], self.layers_structure[i]))\n",
    "\t\t\tself._biases_shapes.append((self.layers_structure[i+1], 1))\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t#? if the parameters is passed to __init__, then set the weights and biases based on that\n",
    "\t\tif isinstance(parameters, np.ndarray):\n",
    "\t\t\t# check the shape before assignment\n",
    "\t\t\tif (s := max(parameters.shape)) != self._NUMBER_OF_PARAMS:\n",
    "\t\t\t\traise ValueError(f'parameters should be of shape ({self._NUMBER_OF_PARAMS}, 1). Got {s} instead.')\n",
    "\t\t\t\n",
    "\t\t\tself.parameters: np.ndarray = parameters\n",
    "\t\t\t\n",
    "\t\t\t# setting both self.weights and self.biases, based on parameters\n",
    "\t\t\tself.parse_parameters()\n",
    "\n",
    "\t\t#? if parameters is not passed to __init__ set them randomly\n",
    "\t\telse:\n",
    "\t\t\tself.weights: list[np.ndarray] = [np.random.randn(*shape) for shape in self._weights_shapes]\n",
    "\t\t\tself.biases: list[np.ndarray] = [np.random.randn(*shape) for shape in self._biases_shapes]\n",
    "\n",
    "\t\t\t# sets self.parameters, based on weights and biases\n",
    "\t\t\tself.recompute_parameters()\n",
    "\n",
    "\t\t\t\n",
    "\t\t#* type of activation\n",
    "\t\t# if only a str, apply it to all layers\n",
    "\t\t# if a list with size L(number_of_layers except input layer), apply individual activations\n",
    "\t\tif isinstance(activation, str):\n",
    "\t\t\tactivation: list[str] = [activation for _ in range(self._L)]\n",
    "\t\t\n",
    "\t\tif len(activation) != self._L:\n",
    "\t\t\traise ValueError(\n",
    "\t\t\t\tf'activation list should be of size [len(layers_structure)-1]={self._L}.\\n Got {len(activation)} instead.'\n",
    "\t\t\t)\n",
    "\t\t\n",
    "\t\tself.activation: list[Callable] = []\n",
    "\t\tself.d_activation: list[Callable] = []\n",
    "\t\t\n",
    "\t\tfor act in activation:\n",
    "\t\t\tmatch act:\n",
    "\t\t\t\tcase 'sigmoid':\n",
    "\t\t\t\t\tself.activation.append(NeuralNetwork._sigmoid)\n",
    "\t\t\t\t\tself.d_activation.append(NeuralNetwork._d_sigmoid)\n",
    "\t\t\t\tcase 'tanh':\n",
    "\t\t\t\t\tself.activation.append(NeuralNetwork._tanh)\n",
    "\t\t\t\t\tself.d_activation.append(NeuralNetwork._d_tanh)\n",
    "\t\t\t\tcase 'relu':\n",
    "\t\t\t\t\tself.activation.append(NeuralNetwork._ReLU)\n",
    "\t\t\t\t\tself.d_activation.append(NeuralNetwork._d_ReLU)\n",
    "\t\t\t\tcase 'no-activation':\n",
    "\t\t\t\t\tself.activation.append(lambda x: x)\n",
    "\t\t\t\t\tself.d_activation.append(lambda x: x)\n",
    "\t\t\t\tcase _:\n",
    "\t\t\t\t\traise ValueError('activations can be a member of [\"sigmoid\", \"relu\", \"tanh\", \"no-activation\"]')\n",
    "\t\t\n",
    "\t\t# initialize all the neurons with zero\n",
    "\t\tself.input_layer: np.ndarray = np.zeros((self.layers_structure[0], 1))\n",
    "\n",
    "\t\t# to initialize all z, and activations\n",
    "\t\tself.z_layers = [None for _ in range(self._L)]\n",
    "\t\tself.layers = [None for _ in range(self._L)]\n",
    "\n",
    "\t\tself.feed_forward()\n",
    "\n",
    "\n",
    "\tdef load_input_layer(self, input_vector: np.ndarray) -> None:\n",
    "\t\t#* input_vector.shape = (self.layers_structure[0], 1)\n",
    "\t\tif input_vector.shape != (self.layers_structure[0], 1):\n",
    "\t\t\traise ValueError(f'input should be of shape {(self.layers_structure[0], 1)}. got {input_vector.shape} instead')\n",
    "\t\t\n",
    "\t\tself.input_layer = input_vector\n",
    "\t\t\n",
    "\n",
    "\tdef cost_of_single_sample(self, sample: np.ndarray, true_label: int) -> float:\n",
    "\t\t#* input_vector.shape = (self.layers_structure[0], 1)\n",
    "\t\tif sample.shape != (self.layers_structure[0], 1):\n",
    "\t\t\traise ValueError(f'input should be of shape {(self.layers_structure[0], 1)}. got {sample.shape} instead')\n",
    "\n",
    "\t\tself.load_input_layer(input_vector=sample)\n",
    "\t\tself.feed_forward()\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t# construct the output vector based on the label\n",
    "\t\tdesired_output = np.zeros((self.layers_structure[-1], 1))\n",
    "\t\tdesired_output[true_label] = 1.0\n",
    "\t\n",
    "\t\t# compare the self.layers[-1] and the desired_output\n",
    "\t\t# using mean squared error\n",
    "\t\tcost = np.sum((self.layers[-1] - desired_output)**2)\n",
    "\t\treturn cost\n",
    "\t\n",
    "\n",
    "\tdef cost_of_test_data(self, test_samples: np.ndarray, true_labels: np.ndarray) -> float:\n",
    "\t\t# samples.shape = (self.layers_structure[0], m)\n",
    "\t \t# true_labels.shape = (1, m)\n",
    "\t\t# samples: is a np array which each col represents one sample\t\t\t\n",
    "\n",
    "\t\tMSE: float = 0\n",
    "\t\tfor sample, label in zip(test_samples.T, true_labels[0]):\n",
    "\t\t\tsample = sample.reshape((-1, 1))\n",
    "\t\t\tMSE += self.cost_of_single_sample(sample, label)\n",
    "\t\t\n",
    "\t\tM = len(test_samples.T)\n",
    "\t\tMSE = (1 / M) * MSE\n",
    "\t\treturn MSE\n",
    "\t\n",
    "\n",
    "\tdef accuracy_score(self, test_samples: np.ndarray, true_labels: np.ndarray) -> float:\n",
    "\t\t\"\"\"\n",
    "\t\tThis method takes a dataset and the corresponding true labels (only the class of the classification problem)\n",
    "\t\tand outputs the accuracy score.\n",
    "\t\tit can only be used with classification problems\n",
    "\t\t\"\"\"\n",
    "\t\t#* test_samples.shape = (self.layer_structure[0], m)\n",
    "\t\t#* true_labels.shape = (1, m)\n",
    "\n",
    "\t\ttotal: int = len(test_samples.T)\n",
    "\t\ttrues: int = 0\n",
    "\n",
    "\t\tfor sample, label in zip(test_samples.T, true_labels[0]):\n",
    "\t\t\tsample = sample.reshape((-1, 1))\n",
    "\t\t\tresult = self.predict_class(sample)\n",
    "\t\t\tif result == label:\n",
    "\t\t\t\ttrues += 1\n",
    "\t\t\n",
    "\t\treturn (trues / total)\n",
    "\n",
    "\n",
    "\tdef predict_output(self, sample: np.ndarray) -> np.ndarray:\n",
    "\t\t\"\"\"\n",
    "\t\tThis method takes in a single sample and outputs the entire output layer.\n",
    "\t\t\"\"\"\n",
    "\t\t#* sample.shape = (self.layers_structure[0], 1)\n",
    "\t\tif sample.shape != (self.layers_structure[0], 1):\n",
    "\t\t\traise ValueError(f'{sample.shape} is a bad shape for input. should be {(self.layers_structure[0], 1)}.')\n",
    "\n",
    "\t\tself.load_input_layer(input_vector=sample)\n",
    "\t\tself.feed_forward()\n",
    "\t\t\n",
    "\t\treturn self.layers[-1]\n",
    "\n",
    "\t\n",
    "\tdef predict_class(self, sample: np.ndarray) -> int:\n",
    "\t\t\"\"\"\n",
    "\t\tThis method takes in a single sample and outputs the index of the highest value in the output layer.\n",
    "\t\t\"\"\"\n",
    "\t\toutput_vector = self.predict_output(sample).flatten()\n",
    "\t\treturn np.argmax(output_vector)\n",
    "\n",
    "\n",
    "\tdef _backprop_one_sample(self, sample: np.ndarray, label: int) -> tuple:\n",
    "\t\t\"\"\"\n",
    "\t\tThis method holds all the math and calculus behind backpropagation\n",
    "\t\tit calculates the derivitive of the cost w.r.t all the weights and\n",
    "\t\tbiases of the network, for only ONE training data\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.load_input_layer(input_vector=sample)\n",
    "\t\tself.feed_forward()\n",
    "\n",
    "\t\t#* convert the label in int format into a one-hot vector\n",
    "\t\tdesired_output = np.zeros(self.layers[-1].shape)\n",
    "\t\tdesired_output[label] = 1.0\n",
    "\t\t\n",
    "\t\t#* d_cost_p_ol.shape = (self.layers_structure[-1], 1)\n",
    "\t\t#* derivative of mean squared error\n",
    "\t\td_cost_p_ol = 2 * (self.layers[-1] - desired_output)\n",
    "\n",
    "\t\t#* d_activation(z_ol)\n",
    "\t\t#* times the gradient of the cost w.r.t activations of the output layer\n",
    "\t\t#* error_ol.shape =(self.layers_structure[-1], 1)\n",
    "\t\terror_ol = self.d_activation[-1](self.z_layers[-1]) *  d_cost_p_ol\n",
    "\t\t\n",
    "\t\t# error of all the other layers, except the last layer and the input layer\n",
    "\t\t#* this errors are gonna be in reverse order, so the first item will be the second to last layer's\n",
    "\t\t#* and the next will be the third from last layer's and so on ...\n",
    "\t\thlayers_errors: list[np.ndarray] = []\n",
    "\n",
    "\t\t\n",
    "\t\t# loop through hidden layers in reverse order, from secnod to last layer, to the second layer\n",
    "\t\t# L-2 is because we should start at the last hidden layer\n",
    "\t\te_count = 0\n",
    "\t\tfor i in range(self._L - 2, -1, -1):\n",
    "\t\t\t#* the layer before the output layer\n",
    "\t\t\tif i == self._L - 2:\n",
    "\t\t\t\te = self.d_activation[i](self.z_layers[i]) * (self.weights[i+1].T @ error_ol)\n",
    "\t\t\t\thlayers_errors.append(e)\n",
    "\t\t\telse:\n",
    "\t\t\t\t#* remember errors[L-2-i] should be used, and it actually means the error of the next layer\n",
    "\t\t\t\t#* this is because it is in the reveresed order\n",
    "\t\t\t\t#e = self.d_activation[i](self.z_layers[i]) * (self.weights[i+1].T @ hlayers_errors[self.L-2-i])\n",
    "\t\t\t\te = self.d_activation[i](self.z_layers[i]) * (self.weights[i+1].T @ hlayers_errors[e_count])\n",
    "\t\t\t\te_count += 1\n",
    "\t\t\t\thlayers_errors.append(e)\n",
    "\n",
    "\t\t#* now we can flip the errors for convenience\n",
    "\t\thlayers_errors = hlayers_errors[::-1]\n",
    "\t\t\n",
    "\t\t# of length L\n",
    "\t\td_cost_p_biases: list[np.ndarray] = []\n",
    "\t\t#* based on the equations of backprpoagation we know that d_cost_p_b of each layer\n",
    "\t\t#* is actually equal to the error of that layer.\n",
    "\t\tfor error in hlayers_errors:\n",
    "\t\t\td_cost_p_biases.append(error)\n",
    "\t\td_cost_p_biases.append(error_ol)\n",
    "\n",
    "\n",
    "\t\t#* based on the equations of backpropagation\n",
    "\t\t#* the derivative of the cost wr to the weights of the layer l will be\n",
    "\t\t#* the matrix mult of error of layer l and activation of layer l-1 transposed\n",
    "\t\td_cost_p_weights: list[np.ndarray] = []\n",
    "\n",
    "\t\td_cost_p_weights.append(hlayers_errors[0] @ self.input_layer.T)\n",
    "\t\tfor i in range(1, self._L - 1):\n",
    "\t\t\td_cost_p_weights.append(error[i] @ self.layers[i-1].T)\n",
    "\t\td_cost_p_weights.append(error_ol @ self.layers[-2].T)\n",
    "\n",
    "\n",
    "\t\treturn (d_cost_p_weights, d_cost_p_biases)\n",
    "\n",
    "\t\n",
    "\tdef backpropagation(self, x_train: np.ndarray, y_train: np.ndarray) -> tuple:\n",
    "\t\t\"\"\"\n",
    "\t\tThis method will run the backprop_one_sample method for a dataset and \n",
    "\t\ttake the average of all the gradients of the weights and biases\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t#* m training samples\n",
    "\t\t#* x_train.shape = (self.layers_structure[0], m)\n",
    "\t\t#* y_train.shape = (1, m)\n",
    "\t\t\n",
    "\t\t# average derivative of cost w.r.t weights\n",
    "\t\tdw: list[np.ndarray] = [np.zeros(shape) for shape in self._weights_shapes]\n",
    "\t\t\n",
    "\t\t# average derivative of cost w.r.t biases\n",
    "\t\tdb: list[np.ndarray] = [np.zeros(shape) for shape in self._biases_shapes]\n",
    "\n",
    "\t\tfor features, label in zip(x_train.T, y_train[0]):\n",
    "\t\t\t#* label: int\n",
    "\t\t\t#* features.shape = (self.layers_structure[0], 1)\n",
    "\t\t\tfeatures = features.reshape((-1, 1))\n",
    "\n",
    "\t\t\t#* label in this method should be an int\n",
    "\t\t\ttdw, tdb = self._backprop_one_sample(sample=features, label=label)\n",
    "\t\t\t\n",
    "\t\t\tfor i in range(self._L):\n",
    "\t\t\t\tdw[i] += tdw[i]\n",
    "\t\t\t\tdb[i] += tdb[i]\n",
    "\t\t\t\n",
    "\t\t#* now each element in the dw and db contain the sum of the derivatives of \n",
    "\t\t#* the samples inside the training data\n",
    "\t\t#* now they should be divided by the number of the train sample size, so dw and db, be an average\n",
    "\t\ttrain_data_size = x_train.shape[1]\n",
    "\t\t\n",
    "\t\tfor i in range(self._L):\n",
    "\t\t\tdw[i] /= train_data_size\n",
    "\t\t\tdb[i] /= train_data_size\n",
    "\n",
    "\t\t#* now they contain the gradient of the provided dataset\n",
    "\t\treturn (dw, db)\n",
    "\t\t\t\n",
    "\n",
    "\tdef recompute_parameters(self) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tThis method will recompute self.parameters from the current self.weights and self.biases\n",
    "\t\t\"\"\"\n",
    "\t\tself.parameters = np.array([])\n",
    "\t\t# first the weights\n",
    "\t\tfor ws in self.weights:\n",
    "\t\t\tself.parameters = np.append(self.parameters, ws.flatten())\n",
    "\t\t# then the biases\n",
    "\t\tfor bs in self.biases:\n",
    "\t\t\tself.parameters = np.append(self.parameters, bs.flatten())\n",
    "\n",
    "\t\t\n",
    "\t\tself.parameters = self.parameters.reshape((-1, 1))\n",
    "\n",
    "\n",
    "\tdef init_weights_biases(self, weights: list[np.ndarray], biases: list[np.ndarray]) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tThis method will take in the weights and biases and sets them in the network.\n",
    "\t\t\"\"\"\n",
    "\t\tif len(weights) != self._L:\n",
    "\t\t\traise ValueError(f'Weights list should contain {self._L} weights matrices!')\n",
    "\t\t\n",
    "\t\tfor i, (ws, shape) in enumerate(zip(weights, self._weights_shapes)):\n",
    "\t\t\tif ws.shape != shape:\n",
    "\t\t\t\traise ValueError(f'{ws.shape} is a wrong shape.(happened in weights[{i}]) should be {shape}.')\n",
    "\n",
    "\t\tfor i, (bs, shape) in enumerate(zip(biases, self._biases_shapes)):\n",
    "\t\t\tif bs.shape != shape:\n",
    "\t\t\t\traise ValueError(f'{bs.shape} is a wrong shape.(happened in biases[{i}]) should be {shape}.')\n",
    "\n",
    "\t\tself.weights: list[np.ndarray] = weights\n",
    "\t\tself.biases: list[np.ndarray] = biases\n",
    "\t\tself.recompute_parameters()\n",
    "\n",
    "\n",
    "\tdef _update_parameters(self, dw, db, learning_rate) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tThis method will update the weights and biases based on the given gradients and learning rate.\n",
    "\t\tbasically applying gradient descent\n",
    "\t\t\"\"\"\n",
    "\t\tself.weights = [\n",
    "\t\t\tweights_matrix - (learning_rate * dw[i])\n",
    "\t\t\tfor i, weights_matrix in enumerate(self.weights)\n",
    "\t\t]\n",
    "\n",
    "\t\tself.biases = [\n",
    "\t\t\tbiases_vector - (learning_rate * db[i])\n",
    "\t\t\tfor i, biases_vector in enumerate(self.biases)\n",
    "\t\t]\n",
    "\n",
    "\t\t#* now self.parameters, which basically is the flattened version of \n",
    "\t\t#* all the weights and biases, should be updated as well\n",
    "\t\tself.recompute_parameters()\n",
    "\n",
    "\n",
    "\tdef parse_parameters(self, parameters: np.ndarray | None = None) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tThis method will parse self.parameters or the given parameters and set each parameter to\n",
    "\t\tthe corresponding weights and biases.\n",
    "\t\t\"\"\"\n",
    "\t\t# if the parameters is passed and is of incorrect shape, stop\n",
    "\t\tif isinstance(parameters, np.ndarray):\n",
    "\t\t\tif parameters.shape != (self._NUMBER_OF_PARAMS, 1):\n",
    "\t\t\t\traise ValueError(f'parameters should be of shape {(self._NUMBER_OF_PARAMS, 1)}.')\n",
    "\t\t\t\n",
    "\t\t\t# update parameters\n",
    "\t\t\tself.parameters = parameters\n",
    "\n",
    "\t\t# otherwise use the current self.parameters\n",
    "\t\t\n",
    "\t\tself.weights: list[np.ndarray] = []\n",
    "\t\tself.biases: list[np.ndarray] = []\n",
    "\t\t\n",
    "\t\t# grab the parameters for weights\n",
    "\t\tcount: int = 0\n",
    "\t\tfor shape in self._weights_shapes:\n",
    "\t\t\ttotal = shape[0] * shape[1]\n",
    "\t\t\tws = self.parameters.T[0][count:count+total].reshape(shape)\n",
    "\t\t\tself.weights.append(ws)\n",
    "\n",
    "\t\t\tcount += total\n",
    "\t\t\n",
    "\t\t# grab the parameters for biases\n",
    "\t\tfor shape in self._biases_shapes:\n",
    "\t\t\ttotal = shape[0]\n",
    "\t\t\tbs = self.parameters.T[0][count:count+total].reshape(shape)\n",
    "\t\t\tself.biases.append(bs)\n",
    "\n",
    "\t\t\tcount += total\n",
    "\n",
    "\n",
    "\tdef train(\n",
    "\t\t\tself, \n",
    "\t\t\tx_train: np.ndarray, \n",
    "\t\t\ty_train: np.ndarray,\n",
    "\t\t\t*,\n",
    "\t\t\tlearning_rate: float = 0.1,\n",
    "\t\t\tconstant_lr: bool = False,\n",
    "\t\t\tdecay_rate: float = 0.1,\n",
    "\t\t\tnumber_of_epochs: int = 80,\n",
    "\t\t\tmini_batches_size: int = 100,\n",
    "\t\t\tquiet: bool = False\n",
    "\t) -> None:\n",
    "\t\t\"\"\"Trains the model with the labeled training data\"\"\"\n",
    "\n",
    "\t\t#* initialize the parameters randomly\n",
    "\t\t#* it is assumed that the parameters are initialized randomly\n",
    "\t\t\n",
    "\t\t#* m training samples\n",
    "\t\t#* x_train.shape = (self.layers_structure[0], m)\n",
    "\t\t#* y_train.shape = (1, m)\n",
    "\t\t\n",
    "\t\t#* first we'd better attach the x_train and y_train together\n",
    "\t\t#* then we can shuffle the training data\n",
    "\t\t#* and then seperate the x and y again\n",
    "\t\t#* add y row to the 0-th row of train_data\n",
    "\t\ttrain_data = np.vstack((y_train, x_train))\n",
    "\n",
    "\t\t#! now because shuffle, shuffles the array based on the rows\n",
    "\t\t#! but we need to shuffle the data based on the coloumns\n",
    "\t\t#! we have to transpose it twice to get around this\n",
    "\t\ttrain_data = train_data.T\n",
    "\t\tnp.random.shuffle(train_data)\n",
    "\t\ttrain_data = train_data.T\n",
    "\n",
    "\t\t#* now that the data is shuffled properly\n",
    "\t\t#* we should divide the data into mini-batches\n",
    "\t\tmini_batches: list[np.ndarray] = [\n",
    "\t\t\ttrain_data[:, count:count+mini_batches_size]\n",
    "\t\t\tfor count in range(0, len(train_data.T), mini_batches_size)\n",
    "\t\t]\n",
    "\n",
    "\n",
    "\t\t#* keep track of the accuracy scores and costs, to plot later\n",
    "\t\tif not quiet:\n",
    "\t\t\tscores: list[float] = []\n",
    "\t\t\tcosts: list[float] = []\n",
    "\n",
    "\t\tinitial_lr: float = learning_rate\n",
    "\t\tfor epoch in range(number_of_epochs):\n",
    "\t\t\t#* now each mini-batch corresponds to one step at gradient descent\n",
    "\t\t\t#* batch.shape = (self.layers_structure[0]+1, mini_batches_size), the label and the features\n",
    "\t\t\tfor batch in mini_batches:\n",
    "\t\t\t\t#* batch.shape = (self.layers_structure[0]+1, mini_batches_size)\n",
    "\t\t\t\tx_train_batch = batch[1:] # (self.layers_structure[0], mini_batches_size)\n",
    "\t\t\t\ty_train_batch = batch[0:1] # (1, mini_batches_size)\n",
    "\n",
    "\t\t\t\t# now because y_train_batch holds the labels, and they should be int\n",
    "\t\t\t\t# int8 is enough because we know labels are single digit ints\n",
    "\t\t\t\ty_train_batch = y_train_batch.astype(np.int8)\n",
    "\n",
    "\t\t\t\t#* the backprop algorithm will run for each batch, one step downhill towards a local minima\n",
    "\t\t\t\t#* it also updates the self.layers[-1](output layer)\n",
    "\t\t\t\t#* and consequently the cost, by running self.feed_forward\n",
    "\t\t\t\tdw, db = self.backpropagation(x_train_batch, y_train_batch)\n",
    "\n",
    "\t\t\t\tif constant_lr:\n",
    "\t\t\t\t\tlr = learning_rate\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tlr = np.exp(-epoch * decay_rate) * initial_lr\n",
    "\n",
    "\n",
    "\t\t\t\t#* change each of the weights and biases accordingly\n",
    "\t\t\t\tself._update_parameters(dw, db, learning_rate=lr)\n",
    "\t\t\t\n",
    "\t\t\t#* parameters are updated now\n",
    "\t\t\tif quiet: continue\n",
    "\n",
    "\t\t\tscore = self.accuracy_score(x_train, y_train)*100\t\n",
    "\t\t\tcost = \tself.cost_of_test_data(x_train, y_train)\n",
    "\t\t\tscores.append(score)\n",
    "\t\t\tcosts.append(cost)\n",
    "\t\t\tprint(f'epoch {epoch+1}:\\taccuracy = {score:.2f}%\\tcost = {cost:.4f}')\n",
    "\n",
    "\t\tif quiet: return\n",
    "\t\tself.plot_scores(number_of_epochs, scores, costs)\n",
    "\n",
    "\t\t\n",
    "\tdef plot_scores(self, num_epochs: int, scores: list[float], costs: list[float]) -> None:\n",
    "\t\tepochs_range =  list(range(1, num_epochs+1))\n",
    "\n",
    "\t\t_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "\t\tax1.set_title('Accuracy Score')\n",
    "\t\tax1.set_ylabel('Accuracy(%)')\n",
    "\t\tax1.set_xlabel('Epoch')\n",
    "\t\tax1.plot(epochs_range, scores)\n",
    "\n",
    "\t\tax2.set_title('Cost')\n",
    "\t\tax2.set_ylabel('Total Cost')\n",
    "\t\tax2.set_xlabel('Epoch')\n",
    "\t\tax2.plot(epochs_range, costs)\n",
    "\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "\t\treturn 1 / (1 + np.exp(-z))\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _d_sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "\t\treturn np.exp(-z) / (np.pow((1 + np.exp(-z)), 2))\n",
    "\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _ReLU(z: np.ndarray) -> np.ndarray:\n",
    "\t\treturn np.maximum(0, z)\n",
    "\t\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef _d_ReLU(z: np.ndarray) -> np.ndarray:\n",
    "\t\treturn (z > 0).astype(np.float64)\n",
    "\t\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _tanh(z: np.ndarray) -> np.ndarray:\n",
    "\t\treturn np.tanh(z)\n",
    "\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef _d_tanh(z: np.ndarray) -> np.ndarray:\n",
    "\t\treturn 4 * np.exp(2 * z) / np.power(np.exp(2*z) + 1, 2)\n",
    "\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef _softmax(z: np.ndarray) -> np.ndarray:\n",
    "\t\treturn np.exp(-z) / np.sum(np.exp(-z))\n",
    "\n",
    "\n",
    "\tdef feed_forward(self) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tWill calculate all the values in all the layers \n",
    "\t\tbased on the weights and biases \n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# loop through each layer(except input layer) and calculate the z of the next layer\n",
    "\t\t# and the activation of the current\n",
    "\t\tfor i in range(self._L):\n",
    "\t\t\t# connection between the input layer and the next layer\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tself.z_layers[i] = (self.weights[0] @  self.input_layer) + self.biases[0]\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.z_layers[i] = (self.weights[i] @ self.layers[i-1]) + self.biases[i]\n",
    "\n",
    "\t\t\tself.layers[i] = self.activation[i](self.z_layers[i])\n",
    "\n",
    "\n",
    "\t\tif self.layers[-1].shape != (self.layers_structure[-1], 1):\n",
    "\t\t\traise ValueError(f'{self.layers[-1].shape} is a bad shape! Should be {(self.layers_structure[-1], 1)}')\n",
    "\n",
    "\n",
    "\tdef print_network(self, hidden_layers = False) -> None:\n",
    "\t\tif hidden_layers:\n",
    "\t\t\t# all hidden layers, all layers except output layer\n",
    "\t\t\tfor i, layer in enumerate(self.layers[:-1]):\n",
    "\t\t\t\tprint(f'layer {i}: {layer}')\n",
    "\n",
    "\t\tprint(f'Output layer:\\n{self.layers[-1]}')\n",
    "\n",
    "\t\t\n",
    "\tdef print_stat(self, x_test: np.ndarray, y_test: np.ndarray) -> None:\n",
    "\t\tscore = self.accuracy_score(x_test, y_test)\n",
    "\t\tcost = self.cost_of_test_data(x_test, y_test)\n",
    "\n",
    "\t\tprint(f'Accuracy = {score * 100:.2f}%')\n",
    "\t\tprint(f'Cost = {cost:.3f}')\n",
    "\t\n",
    "\n",
    "\tdef\tload_params_from_file(self, filename: str) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tload parameters from a saved parameters file\n",
    "\t\t\"\"\"\n",
    "\t\twith open(filename, 'r') as file:\n",
    "\t\t\tps = [float(line.strip()) for line in file.readlines()]\n",
    "\n",
    "\t\tself.parse_parameters(np.array(ps).reshape((-1, 1)))\n",
    "\n",
    "\n",
    "\tdef save_parameters_to_file(self, filename: str | None = None) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tsaves current network parameters to a file\n",
    "\t\t\"\"\"\n",
    "\t\tif not filename:\n",
    "\t\t\tfilename = f\"parameters_{datetime.now().strftime('%y_%m_%d_%H_%M')}.txt\"\n",
    "\t\twith open(filename, 'w') as file:\n",
    "\t\t\tfor p in self.parameters.reshape((-1, )):\n",
    "\t\t\t\tfile.write(f'{p}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "First of all you need to prepare the dataset in csv format. \n",
    "You can visit [this link](https://www.kaggle.com/datasets/hojjatk/mnist-dataset) on kaggle and download the zip file.\n",
    "\n",
    "Then you need to extract the zip file, and place these four files in the `data` directory. (as the same level as the `generate_mnist_csv.py` file)\n",
    "- t10k-images.idx3-ubyte\n",
    "- t10k-labels.idx1-ubyte\n",
    "- train-images.idx3-ubyte\n",
    "- train-labels.idx1-ubyte\n",
    "\n",
    "Now when you run `generate_mnist_csv.py` file, two csv files should be created.\n",
    "\n",
    "Alternativly, you can use any other data that you have in mind. this is just an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/mnist_train.csv')\n",
    "test_df = pd.read_csv('./data/mnist_test.csv')\n",
    "\n",
    "x_train = train_df.iloc[:, 1:].transpose().values # shape = 784 * m, so each col is a sample\n",
    "x_train = (x_train / 255.0) # to squish the pixel values between 0-1 instead of 0-255\n",
    "y_train = train_df.iloc[:, 0:1].values.reshape((1, -1)) # shape = 1 * m\n",
    "\n",
    "x_test = test_df.iloc[:, 1:].values.transpose()\n",
    "x_test = (x_test / 255.0)\n",
    "y_test = test_df.iloc[:, 0:1].values.reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "ps = np.random.randn(13002, 1)\n",
    "NN = NeuralNetwork(layers_structure=[784, 16, 16, 10], parameters=ps, activation='sigmoid')\n",
    "NN.parse_parameters(parameters=ps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.print_stat(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.train(x_train, y_train, number_of_epochs=3, mini_batches_size=120, learning_rate=1, constant_lr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.print_stat(x_train, y_train)\n",
    "NN.print_stat(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_df = pd.read_csv('./data/self_handwritten_digits/custom_data.csv')\n",
    "custom_test_x = np.array(custom_df).T\n",
    "custom_test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(sample: np.ndarray):\n",
    "\t\t#* sample.shape = (784, 1)\n",
    "\t\tmatrix = np.array(sample).reshape(28, 28)\n",
    "\t\tplt.imshow(matrix, cmap='gray', vmin=0, vmax=255)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.show()\n",
    "\n",
    "\n",
    "for sample in custom_test_x.T:\n",
    "\tsample = sample.reshape((-1, 1))\n",
    "\tprint(NN.predict_class(sample))\n",
    "\tshow_sample(sample)\n",
    "\n",
    "NN.accuracy_score(custom_test_x, [list(range(10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* examples where the prediction was not correct\n",
    "for i, (sample, label) in enumerate(zip(x_test.T, y_test[0])):\n",
    "\tsample = sample.reshape((-1, 1))\n",
    "\tp = NN.predict_class(sample)\n",
    "\tif p != label:\n",
    "\t\tshow_sample(sample)\n",
    "\t\tprint(f'true label: {label}')\n",
    "\t\tprint(f'prediction: {p}')\n",
    "\tif i >= 20: break\t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
