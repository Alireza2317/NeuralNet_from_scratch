{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Implementation From Scratch\n",
    "### Getting Started\n",
    "The very first step would be to create a virtual enviornment and install all the requirements. You can do so by running these commands:\n",
    "\n",
    "```bash\n",
    "virtualenv venv\n",
    "source venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Then you can come back to this notebook and run the codes. \n",
    "\n",
    "(More information on the projects github's page [here](https://github.com/Alireza2317/NeuralNet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\t\"\"\"\n",
    "\tThis class is specificly desgined for mnist handwritten digits recognition, with 784 neurons in\n",
    "\tthe input layer, and 10 neurons in the output layer, representing each one of the digits\n",
    "\tthe network contains 2 hidden layers, each consisting of 16 neurons.\n",
    "\tit can also handle different activations such as sigmoid, ReLU and tanh.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, parameters: np.ndarray = np.zeros((13002, 1)), activation: str = 'sigmoid') -> None:\n",
    "\t\tif parameters.shape != (13002, 1):\n",
    "\t\t\traise ValueError('parameters shape should be (13002, 1)!')\n",
    "\t\t\n",
    "\t\tself.parameters = parameters\n",
    "\n",
    "\t\tcount = 0\n",
    "\t\t\n",
    "\t\t# weights1 is of size (16)x(28*28)\n",
    "\t\ttotal_size = 16*28*28\n",
    "\t\tself.weights1 = parameters[count:count+total_size].reshape(16, 28*28)\n",
    "\n",
    "\t\tcount += total_size \n",
    "\n",
    "\t\t# weights2 is of size 16x16\n",
    "\t\ttotal_size = 16*16\n",
    "\t\tself.weights2 = parameters[count:count+total_size].reshape(16, 16)\n",
    "\n",
    "\t\tcount += total_size \n",
    "\n",
    "\t\t# weights3 is of size 10*16\n",
    "\t\ttotal_size = 10*16\n",
    "\t\tself.weights3 = parameters[count:count+total_size].reshape(10, 16)\n",
    "\n",
    "\t\tcount += total_size \n",
    "\n",
    "\t\t# biases1 is of size 16x1\n",
    "\t\ttotal_size = 16*1\n",
    "\t\tself.biases1 = parameters[count:count+total_size].reshape(16, 1)\n",
    "\n",
    "\t\tcount += total_size \n",
    "\n",
    "\t\t# biases2 is of size 16x1\n",
    "\t\ttotal_size = 16*1\n",
    "\t\tself.biases2 = parameters[count:count+total_size].reshape(16, 1)\n",
    "\t\t\n",
    "\t\tcount += total_size \n",
    "\n",
    "\t\t# biases3 is of size 10*1\n",
    "\t\ttotal_size = 10*1\n",
    "\t\tself.biases3 = parameters[count:count+total_size].reshape(10, 1)\n",
    "\n",
    "\t\t# type of activation\n",
    "\t\tmatch activation:\n",
    "\t\t\tcase 'sigmoid':\n",
    "\t\t\t\tself.activation: Callable = NeuralNetwork.sigmoid\n",
    "\t\t\t\tself.d_activation: Callable = NeuralNetwork.d_sigmoid\n",
    "\t\t\tcase 'relu':\n",
    "\t\t\t\tself.activation: Callable = NeuralNetwork.ReLU\n",
    "\t\t\t\tself.d_activation: Callable = NeuralNetwork.d_ReLU\n",
    "\t\t\tcase 'tanh':\n",
    "\t\t\t\tself.activation: Callable = NeuralNetwork.tanh\n",
    "\t\t\t\tself.d_activation: Callable = NeuralNetwork.d_tanh\n",
    "\t\t\tcase 'no-activation':\n",
    "\t\t\t\tself.activation: Callable = lambda x: x\n",
    "\t\t\t\tself.d_activation: Callable = lambda x: x\n",
    "\n",
    "\t\tself.input_layer = np.zeros((784, 1))\n",
    "\t\t\n",
    "\t\t# inner hidden layers of the network\n",
    "\t\tself.z_hlayer1 = np.zeros((16, 1))\n",
    "\t\tself.hlayer1 = self.activation(self.z_hlayer1)\n",
    "\n",
    "\t\tself.z_hlayer2 = np.zeros((16, 1))\n",
    "\t\tself.hlayer2 = self.activation(self.z_hlayer2)\n",
    "\n",
    "\t\tself.z_output_layer = np.zeros((10, 1))\n",
    "\t\tself.output_layer = self.activation(self.z_output_layer)\n",
    "\n",
    "\t\tdel total_size\n",
    "\t\tdel count\n",
    "\n",
    "\n",
    "\t\n",
    "\tdef load_input_layer(self, input_vector: np.ndarray) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tLaad the input handwritten digit\n",
    "\t\tinput_vector: np.ndarray of shape (784, 1)\n",
    "\t\tthese numbers are between 0-255\n",
    "\t\tthe function will squish them between 0-1\n",
    "\t\t\"\"\"\n",
    "\t\tself.input_layer = (input_vector / 255)\n",
    "\t\t\n",
    "\n",
    "\tdef cost_of_single_sample(self, sample: np.ndarray, true_label: int) -> float:\n",
    "\t\tself.load_input_layer(input_vector=sample)\n",
    "\t\tself.feed_forward()\n",
    "\t\t\n",
    "\t\t# construct the output vector based on the label\n",
    "\t\tdesired_output = np.zeros((10, 1))\n",
    "\t\tdesired_output[true_label] = 1.0\n",
    "\t\t\n",
    "\t\t# compare the self.output_layer and the desired_output\n",
    "\t\t# using mean squared error\n",
    "\t\tcost = np.sum((self.output_layer - desired_output)**2)\n",
    "\t\treturn cost\n",
    "\t\n",
    "\n",
    "\tdef cost_of_test_data(self, test_samples: np.ndarray, true_labels: np.ndarray) -> float:\n",
    "\t\t\"\"\"\n",
    "\t\tsamples.shape = (784, m)\n",
    "\t\ttrue_labels.shape = (1, m)\n",
    "\t\tsamples: is a np array which each col represents one sample, and \n",
    "\t\teach col has 784 numbers in them, the pixel values\n",
    "\t\t\t\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tMSE: float = 0\n",
    "\t\tfor sample, label in zip(test_samples.T, true_labels[0]):\n",
    "\t\t\tMSE += self.cost_of_single_sample(sample, label)\n",
    "\t\t\n",
    "\t\tM = len(test_samples.T)\n",
    "\t\tMSE = (1 / M) * MSE\n",
    "\t\treturn MSE\n",
    "\t\n",
    "\n",
    "\tdef accuracy_score(self, test_samples: np.ndarray, true_labels: np.ndarray) -> float:\n",
    "\t\t#* test_samples.shape = (784, m)\n",
    "\t\t#* true_labels.shape = (1, m)\n",
    "\n",
    "\t\ttotal: int = len(test_samples.T)\n",
    "\t\ttrues: int = 0\n",
    "\n",
    "\t\tfor sample, label in zip(test_samples.T, true_labels[0]):\n",
    "\t\t\tresult = self.predict(sample)\n",
    "\t\t\tif result == label:\n",
    "\t\t\t\ttrues += 1\n",
    "\t\t\n",
    "\t\treturn (trues / total)\n",
    "\n",
    "\n",
    "\tdef predict_v(self, sample: np.ndarray) -> np.ndarray:\n",
    "\t\t#* sample.shape = (784, 1)\n",
    "\t\tself.load_input_layer(input_vector=sample)\n",
    "\t\tself.feed_forward()\n",
    "\t\t\n",
    "\t\treturn self.output_layer\n",
    "\n",
    "\t\n",
    "\tdef predict(self, sample) -> int:\n",
    "\t\toutput_vector = self.predict_v(sample)\n",
    "\t\treturn np.argmax(output_vector)\n",
    "\n",
    "\n",
    "\tdef backprop_one_sample(self, sample: np.ndarray, label: int) -> tuple:\n",
    "\t\t\"\"\"\n",
    "\t\tThis method holds all the math and calculus behind backpropagation\n",
    "\t\tit calculates the derivitive of the cost w.r.t all the weights and\n",
    "\t\tbiases of the network, for only one training data\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\tself.load_input_layer(input_vector=sample)\n",
    "\t\tself.feed_forward()\n",
    "\n",
    "\t\t#* convert the label in int format into a one-hot vector\n",
    "\t\tdesired_output = np.zeros(self.output_layer.shape)\n",
    "\t\tdesired_output[label] = 1.0\n",
    "\t\t\n",
    "\t\t#* d_cost_p_ol.shape = (10, 1)\n",
    "\t\td_cost_p_ol = 2 * (self.output_layer - desired_output)\n",
    "\n",
    "\t\t#* d_activation(z_ol)\n",
    "\t\t#* times the gradient of the cost w.r.t activations of the output layer\n",
    "\t\t#* error_ol.shape = (10, 1)\n",
    "\t\terror_ol = self.d_activation(self.z_output_layer) *  d_cost_p_ol\n",
    "\n",
    "\t\t#* d_activation(self.z_hlayer2).shape = (16, 1)\n",
    "\t\t#* self.weights3.T.shape = (10, 16).T -> (16, 10)\n",
    "\t\t#* error_ol.shape = (10, 1)\n",
    "\t\t#* error_hl2.shape = (16, 1)\n",
    "\t\terror_hl2 = self.d_activation(self.z_hlayer2) * (self.weights3.T @ error_ol)\n",
    "\n",
    "\n",
    "\t\t#* d_activation(self.z_hlayer1).shape = (16, 1)\n",
    "\t\t#* self.weights2.T.shape = (16, 16).T -> (16, 16)\n",
    "\t\t#* error_hl2.shape = (16, 1)\n",
    "\t\t#* error_hl1.shape = (16, 1)\n",
    "\t\terror_hl1 = self.d_activation(self.z_hlayer1) * (self.weights2.T @ error_hl2)\n",
    "\t\t\n",
    "\t\t#* d_cost_p_b3.shape = output_layer.shape = (10, 1)\n",
    "\t\td_cost_p_b3 = error_ol\n",
    "\t\n",
    "\n",
    "\t\t#* d_cost_p_b2.shape = hlayer2.shape = (16, 1)\n",
    "\t\td_cost_p_b2 = error_hl2\n",
    "\n",
    "\n",
    "\t\t#* d_cost_p_b1.shape = hlayer1.shape = (16, 1)\n",
    "\t\td_cost_p_b1 = error_hl1\n",
    "\n",
    "\t\t#* the derivative of the cost wr to the weights of the layer l will be\n",
    "\t\t#* the matrix mult of error of layer l and activation of layer l-1 transposed\n",
    "\t\t\n",
    "\t\t#* error_ol.shape = (10, 1)\n",
    "\t\t#* self.hlayer2.T.shape = (16, 1).T = (1, 16)\n",
    "\t\t#* d_cost_p_w3.shape = (10, 16)\n",
    "\t\td_cost_p_w3 = (error_ol @ self.hlayer2.T)\n",
    "\t\t\n",
    "\n",
    "\t\t#* error_hl2.shape = (16, 1)\n",
    "\t\t#* self.hlayer1.T.shape = (16, 1).T = (1, 16)\n",
    "\t\t#* d_cost_p_w2.shape = (16, 16)\n",
    "\t\td_cost_p_w2 = (error_hl2 @ self.hlayer1.T)\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t#* error_hl1.shape = (16, 1)\n",
    "\t\t#* self.input_layer.T.shape = (784, 1).T = (1, 784)\n",
    "\t\t#* d_cost_p_w1.shape = (16, 784)\n",
    "\t\td_cost_p_w1 = (error_hl1 @ self.input_layer.T)\n",
    "\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\td_cost_p_w1,\n",
    "\t\t\td_cost_p_b1,\n",
    "\t\t\td_cost_p_w2,\n",
    "\t\t\td_cost_p_b2,\n",
    "\t\t\td_cost_p_w3,\n",
    "\t\t\td_cost_p_b3\n",
    "\t\t)\n",
    "\n",
    "\t\n",
    "\tdef backpropagation(self, x_train: np.ndarray, y_train: np.ndarray) -> tuple:\n",
    "\t\t\"\"\"\n",
    "\t\tThis method will run the backprop_one_sample method for a dataset and \n",
    "\t\ttake the average of all the gradients of the weights and biases\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t#* m training samples\n",
    "\t\t#* x_train.shape = (784, m)\n",
    "\t\t#* y_train.shape = (1, m)\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tdw1 = np.zeros(self.weights1.shape)\n",
    "\t\tdb1 = np.zeros(self.biases1.shape)\n",
    "\t\t\n",
    "\t\tdw2 = np.zeros(self.weights2.shape)\n",
    "\t\tdb2 = np.zeros(self.biases2.shape)\n",
    "\t\t\n",
    "\t\tdw3 = np.zeros(self.weights3.shape)\n",
    "\t\tdb3 = np.zeros(self.biases3.shape)\n",
    "\n",
    "\t\t\n",
    "\t\tfor features, label in zip(x_train.T, y_train[0]):\n",
    "\t\t\t#* label: int\n",
    "\t\t\t#* features.shape = (784, 1)\n",
    "\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t#* label in this method should be an int\n",
    "\t\t\t(\n",
    "\t\t\t\ttdw1, \n",
    "\t\t\t\ttdb1, \n",
    "\t\t\t\ttdw2, \n",
    "\t\t\t\ttdb2, \n",
    "\t\t\t\ttdw3, \n",
    "\t\t\t\ttdb3\n",
    "\t\t\t) = self.backprop_one_sample(sample=features, label=label)\n",
    "\t\t\t\n",
    "\t\t\tdw1 += tdw1\n",
    "\t\t\tdb1 += tdb1\n",
    "\t\t\tdw2 += tdw2\n",
    "\t\t\tdb2 += tdb2\n",
    "\t\t\tdw3 += tdw3\n",
    "\t\t\tdb3 += tdb3\n",
    "\n",
    "\t\t#* now the dw1, db1, dw2, db2, dw3, db3 contain the sum of the derivatives of \n",
    "\t\t#* the samples inside the training data\n",
    "\t\t#* now they should be divided by the number of the train sample size, so dw and db, be an average\n",
    "\t\ttrain_data_size = x_train.shape[1]\n",
    "\t\t\n",
    "\t\tdw1 /= train_data_size\n",
    "\t\tdb1 /= train_data_size\n",
    "\t\tdw2 /= train_data_size\n",
    "\t\tdb2 /= train_data_size\n",
    "\t\tdw3 /= train_data_size\n",
    "\t\tdb3 /= train_data_size\n",
    "\t\t\n",
    "\t\t#* now they contain the gradient of the provided dataset\n",
    "\t\treturn (\n",
    "\t\t\tdw1,\n",
    "\t\t\tdb1,\n",
    "\t\t\tdw2,\n",
    "\t\t\tdb2,\n",
    "\t\t\tdw3,\n",
    "\t\t\tdb3\n",
    "\t\t)\n",
    "\t\t\t\n",
    "\n",
    "\tdef update_parameters(self, dw1, db1, dw2, db2, dw3, db3, learning_rate) -> None:\n",
    "\t\tself.weights1 -= learning_rate * dw1\n",
    "\t\tself.biases1 -= learning_rate * db1\n",
    "\t\t\n",
    "\t\tself.weights2 -= learning_rate * dw2\n",
    "\t\tself.biases2 -= learning_rate * db2\n",
    "\n",
    "\t\tself.weights3 -= learning_rate * dw3\n",
    "\t\tself.biases3 -= learning_rate * db3\n",
    "\n",
    "\n",
    "\tdef train(\n",
    "\t\t\tself, \n",
    "\t\t\tx_train: np.ndarray, \n",
    "\t\t\ty_train: np.ndarray,\n",
    "\t\t\t*,\n",
    "\t\t\tlearning_rate: float = 0.1,\n",
    "\t\t\tconstant_lr: bool = False,\n",
    "\t\t\tnumber_of_epochs: int = 80,\n",
    "\t\t\tmini_batches_size: int = 100\n",
    "\t) -> None:\n",
    "\t\t\"\"\"Trains the model with the labeled training data\"\"\"\n",
    "\t\t\n",
    "\t\t#* m training samples\n",
    "\t\t#* x_train.shape = (784, m)\n",
    "\t\t#* y_train.shape = (1, m)\n",
    "\t\t\n",
    "\t\t#* first we'd better attach the x_train and y_train together\n",
    "\t\t#* then we can shuffle the training data\n",
    "\t\t#* and then seperate the x and y again\n",
    "\t\t#* add y row to the 0-th row of train_data\n",
    "\t\ttrain_data = np.vstack((y_train, x_train))\n",
    "\n",
    "\t\t#! now because shuffle, shuffles the array based on the rows\n",
    "\t\t#! but we need to shuffle the data based on the coloumns\n",
    "\t\t#! we have to transpose it twice to get around this\n",
    "\t\ttrain_data = train_data.T\n",
    "\t\tnp.random.shuffle(train_data)\n",
    "\t\ttrain_data = train_data.T\n",
    "\n",
    "\t\t#* now that the data is shuffled properly\n",
    "\t\t#* we should divide the data into mini-batches\n",
    "\t\tmini_batches: list[np.ndarray] = []\n",
    "\t\tcount: int = 0\n",
    "\t\twhile True:\n",
    "\t\t\tif count >= len(train_data.T):\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\tmini_batches.append(train_data[:, count:count+mini_batches_size])\n",
    "\t\t\tcount += mini_batches_size\n",
    "\n",
    "\t\t#* keep track of the accuracy scores, to plot later\t\n",
    "\t\tscores: list[float] = []\n",
    "\n",
    "\t\tinitial_lr: float = learning_rate\n",
    "\t\tdecay_rate: float = 0.1\n",
    "\t\tfor epoch in range(number_of_epochs):\n",
    "\t\t\t#* now each mini-batch corresponds to one step at gradient descent\n",
    "\t\t\t#* batch.shape = (784+1, mini_batches_size), the label and the features\n",
    "\t\t\tfor batch in mini_batches:\n",
    "\t\t\t\t#* batch.shape = (785, mini_batches_size)\n",
    "\t\t\t\tx_train_batch = batch[1:] # (784, mini_batches_size)\n",
    "\t\t\t\ty_train_batch = batch[0:1] # (1, mini_batches_size)\n",
    "\t\t\t\t\n",
    "\t\t\t\t#* the backprop algorithm will run for each batch, one step downhill towards a local minima\n",
    "\t\t\t\t#* it also updates the self.output_layer and consequently the cost by running self.feed_forward\n",
    "\t\t\t\tdw1, db1, dw2, db2, dw3, db3 = self.backpropagation(x_train_batch, y_train_batch)\n",
    "\n",
    "\t\t\t\tif constant_lr:\n",
    "\t\t\t\t\tlr = learning_rate\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tlr = np.exp(-epoch * decay_rate) * initial_lr\n",
    "\n",
    "\n",
    "\t\t\t\t#* change each of the weights and biases accordingly\n",
    "\t\t\t\tself.update_parameters(dw1, db1, dw2, db2, dw3, db3, learning_rate=lr)\n",
    "\t\t\tscore = self.accuracy_score(x_train, y_train)*100\t\t\t\t\n",
    "\t\t\tscores.append(score)\n",
    "\t\t\tprint(f'epoch {epoch+1}: {score:.2f}%')\n",
    "\n",
    "\t\tself.plot_scores(list(range(1, number_of_epochs+1)), scores)\n",
    "\n",
    "\t\t\n",
    "\tdef plot_scores(self, epochs_range: list[int], scores: list[float]) -> None:\n",
    "\t\tfig = plt.figure()\n",
    "\t\tfig.tight_layout()\n",
    "\t\tplt.title('Perfecting Accuracy Score')\n",
    "\t\tplt.xlabel('Epoch')\n",
    "\t\tplt.ylabel('Accuracy Score')\n",
    "\t\tplt.xticks(epochs_range)\n",
    "\t\tplt.plot(epochs_range, scores)\n",
    "\t\tplt.show()\n",
    "\t\t\t\t\n",
    "\t@staticmethod\n",
    "\tdef sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "\t\treturn 1 / (1 + np.exp(-z))\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef d_sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "\t\treturn np.exp(-z) / (np.pow((1 + np.exp(-z)), 2))\n",
    "\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef ReLU(z: np.ndarray) -> np.ndarray:\n",
    "\t\treturn np.maximum(0, z)\n",
    "\t\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef d_ReLU(z: np.ndarray) -> np.ndarray:\n",
    "\t\treturn (z > 0).astype(np.float64)\n",
    "\t\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef tanh(z: np.ndarray) -> np.ndarray:\n",
    "\t\treturn np.tanh(z)\n",
    "\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef d_tanh(z: np.ndarray) -> np.ndarray:\n",
    "\t\treturn 4 * np.exp(2 * z) / np.power(np.exp(2*z) + 1, 2)\n",
    "\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef softmax(z: np.ndarray) -> np.ndarray:\n",
    "\t\treturn np.exp(-z) / np.sum(np.exp(-z))\n",
    "\n",
    "\n",
    "\tdef feed_forward(self) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tWill calculate all the values in all the layers \n",
    "\t\tbased on the weights and biases \n",
    "\t\t\"\"\"\n",
    "\t\t# hidden layer 1 has 16 neurons -> shape: (16, 1)\n",
    "\t\t# weights1.shape = (16, 784)\n",
    "\t\t# 16*784\t784*1 + 16,1\n",
    "\t\tself.input_layer = self.input_layer.reshape((784, 1))\n",
    "\n",
    "\t\tself.z_hlayer1 = self.weights1 @ self.input_layer + self.biases1\n",
    "\t\tself.hlayer1 = self.activation(self.z_hlayer1)\n",
    "\n",
    "\t\t# hidden layer 2 has 16 neurons -> shape: (16, 1)\n",
    "\t\tself.z_hlayer2 = self.weights2 @ self.hlayer1 + self.biases2\n",
    "\t\tself.hlayer2 = self.activation(self.z_hlayer2)\n",
    "\t\t\n",
    "\t\t# output layer has 10 neurons -> shape(10, 1)\n",
    "\t\t# one neuron for each digit\n",
    "\t\tself.z_output_layer = self.weights3 @ self.hlayer2 + self.biases3\n",
    "\t\tself.output_layer = self.activation(self.z_output_layer)\n",
    "\n",
    "\n",
    "\t\tif self.output_layer.shape != (10, 1):\n",
    "\t\t\traise ValueError(f'{self.output_layer.shape} not a correct shape!')\n",
    "\n",
    "\n",
    "\tdef print_network(self, hidden_layers = False) -> None:\n",
    "\t\tif hidden_layers:\n",
    "\t\t\tprint('hLayer 1:')\n",
    "\t\t\tprint(self.hlayer1)\n",
    "\t\t\t\n",
    "\t\t\tprint('hLayer 2:')\n",
    "\t\t\tprint(self.hlayer2)\n",
    "\n",
    "\t\tprint('Output Layer:')\n",
    "\t\tprint(self.output_layer)\n",
    "\n",
    "\n",
    "\tdef plot_network(self):\n",
    "\t\tlayers = [\n",
    "\t\t\tself.hlayer1.reshape((16,)),\n",
    "\t\t\tself.hlayer2.reshape((16,)),\n",
    "\t\t\tself.output_layer.reshape((10,))\t\t\n",
    "\t\t]\n",
    "\n",
    "\t\tfig, ax = plt.subplots()\n",
    "\t\t\n",
    "\t\t# Determine the positions for the layers\n",
    "\t\tlayer_sizes = [len(layer) for layer in layers]\n",
    "\t\tv_spacing = max(layer_sizes) + 1 + 3  # Vertical spacing between neurons\n",
    "\t\th_spacing = 15  # Horizontal spacing between layers\n",
    "\t\t\n",
    "\t\t# Iterate over layers\n",
    "\t\tfor i, layer in enumerate(layers):\n",
    "\t\t\ty_pos = np.linspace(v_spacing / 2, -v_spacing / 2, len(layer))\n",
    "\t\t\tx_pos = np.full_like(y_pos, i * h_spacing)\n",
    "\t\t\t\n",
    "\t\t\t# Plot neurons\n",
    "\t\t\tfor j, neuron_value in enumerate(layer):\n",
    "\t\t\t\tcircle = plt.Circle((x_pos[j], y_pos[j]), radius=0.4, facecolor=str(neuron_value), edgecolor='k')\n",
    "\t\t\t\tax.add_artist(circle)\n",
    "\t\t\t\t# Optional: Add text to show the neuron value\n",
    "\t\t\t\tax.text(x_pos[j]+1.5, y_pos[j], f'{neuron_value:.2f}', ha='center', va='center', color='black')\n",
    "\n",
    "\t\t\t# Draw connections (optional)\n",
    "\t\t\tif i > 0:\n",
    "\t\t\t\tprev_layer = layers[i - 1]\n",
    "\t\t\t\tprev_y_pos = np.linspace(v_spacing / 2, -v_spacing / 2, len(prev_layer))\n",
    "\t\t\t\tprev_x_pos = np.full_like(prev_y_pos, (i - 1) * h_spacing)\n",
    "\t\t\t\tfor k in range(len(prev_layer)):\n",
    "\t\t\t\t\tfor l in range(len(layer)):\n",
    "\t\t\t\t\t\tax.plot([prev_x_pos[k], x_pos[l]], [prev_y_pos[k], y_pos[l]], 'k-', lw=0.04)\n",
    "\n",
    "\t\tax.set_aspect('equal')\n",
    "\t\tax.axis('off')\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.show()\n",
    "\n",
    "\n",
    "\tdef show_sample(self, sample: np.ndarray):\n",
    "\t\t#* sample.shape = (784, 1)\n",
    "\t\tmatrix = np.array(sample).reshape(28, 28)\n",
    "\n",
    "\t\tplt.imshow(matrix, cmap='gray', vmin=0, vmax=255)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\n",
    "\tdef print_stat(self, x_test: np.ndarray, y_test: np.ndarray) -> None:\n",
    "\t\tscore = self.accuracy_score(x_test, y_test)\n",
    "\t\tcost = self.cost_of_test_data(x_test, y_test)\n",
    "\n",
    "\t\tprint(f'Accuracy = {score * 100:.2f}%')\n",
    "\t\tprint(f'Cost = {cost:.3f}')\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "First of all you need to prepare the dataset in csv format. \n",
    "You can visit [this link](https://www.kaggle.com/datasets/hojjatk/mnist-dataset) on kaggle and download the zip file.\n",
    "\n",
    "Then you need to extract the zip file, and place these four files in the `data` directory. (as the same level as the `generate_mnist_csv.py` file)\n",
    "- t10k-images.idx3-ubyte\n",
    "- t10k-labels.idx1-ubyte\n",
    "- train-images.idx3-ubyte\n",
    "- train-labels.idx1-ubyte\n",
    "\n",
    "Now when you run `generate_mnist_csv.py` file, two csv files should be created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "!python3 ./data/generate_mnist_csv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/mnist_train.csv')\n",
    "test_df = pd.read_csv('./data/mnist_test.csv')\n",
    "\n",
    "x_train = np.array(train_df.iloc[:, 1:]).transpose() # shape = 784 * m, so each col is a sample\n",
    "y_train = np.array(train_df.iloc[:, 0:1]).reshape((1, -1)) # shape = 1 * m\n",
    "\n",
    "x_test = np.array(test_df.iloc[:, 1:]).transpose()\n",
    "y_test = np.array(test_df.iloc[:, 0:1]).reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* initialize the parameters randomly\n",
    "ps = np.random.normal(size=(13002, 1)) / 2\n",
    "\t\t\n",
    "NN = NeuralNetwork(parameters=ps, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.print_stat(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.train(x_train, y_train, number_of_epochs=5, mini_batches_size=150, learning_rate=1, constant_lr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.print_stat(x_train, y_train)\n",
    "NN.print_stat(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_df = pd.read_csv('./data/self_handwritten_digits/custom_data.csv')\n",
    "custom_test_x = np.array(custom_df).T\n",
    "\n",
    "for i, sample in enumerate(custom_test_x.T):\n",
    "\tpred = NN.predict(sample)\n",
    "\tprint(f'true = {i}')\n",
    "\tprint(f'prediction = {pred}')\n",
    "\tif i == pred:\n",
    "\t\tprint('CORRECT!')\n",
    "\telse:\n",
    "\t\tprint('WRONG!')\n",
    "\t\n",
    "\tNN.show_sample(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* examples where the prediction was not correct\n",
    "count = 0\n",
    "for i, (sample, label) in enumerate(zip(x_test.T, y_test[0])):\n",
    "\tp = NN.predict(sample)\n",
    "\tif p != label:\n",
    "\t\tNN.show_sample(sample)\n",
    "\t\tprint(f'true label: {label}')\n",
    "\t\tprint(f'prediction: {p}')\n",
    "\t\tcount += 1\n",
    "\t\tif count == 10:\n",
    "\t\t\tbreak\n",
    "\n",
    "layers = [NN.hlayer1.reshape((16,)), NN.hlayer2.reshape((16,)), NN.output_layer.reshape((10,))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
